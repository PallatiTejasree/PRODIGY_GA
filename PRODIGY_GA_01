import os
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_dataset, Dataset

# Configuration
MODEL_NAME = 'gpt2'
DATA_FILE = 'data.txt'
OUTPUT_DIR = './gpt2-finetuned'
EPOCHS = 3
BATCH_SIZE = 4
LEARNING_RATE = 2e-5

# Load and preprocess the dataset
def load_and_preprocess_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # Create a Dataset object
    dataset = Dataset.from_dict({"text": lines})
    return dataset

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)
model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)

# Load and preprocess dataset
dataset = load_and_preprocess_data(DATA_FILE)
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Set up training arguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    save_steps=10_000,
    save_total_limit=2,
    learning_rate=LEARNING_RATE,
    logging_dir='./logs',
    logging_steps=500,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Save the model and tokenizer
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"Model and tokenizer saved to {OUTPUT_DIR}")

# Generate text using the fine-tuned model
def generate_text(prompt, max_length=50):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Example text generation
prompt = "Once upon a time"
generated_text = generate_text(prompt)
print(f"Generated text:\n{generated_text}")
